# Azure Data Factory Triggers with Terraform

This repository allows the configuration of Azure Data Factory (ADF) triggers.

ADF uses these triggers to schedule pipeline runs. These pipelines are created
outside of this repository. Code for the pipelines themselves is managed in
[this repository](https://dev.azure.com/NIBC/Data%20Platform/_git/adf-cdp-pipelines),
and is generated by using the ADF User Interface (UI).

These pipelines (and underlying objects in ADF) are created in such a way that
they are re-usable, so we can use the same pipeline multiple times for similar
datasets. For example, for multiple tables coming from the same database, we can re-use
the same pipeline. And even for multiple datastores, if they share the same type.

This recycling of the same pipeline is made possible by parameterizing the pipeline objects
and creating a separate trigger for each dataset, providing values for the pipeline
parameters specific to that dataset.

- [How to schedule a new pipeline?](#how-to-schedule-a-new-pipeline)
  - [Using scheduled triggers](#using-scheduled-triggers)
    - [Creating a parameterized pipeline](#creating-a-parameterized-pipeline)
      - [Required parameters for each pipeline](#required-parameters-for-each-pipeline)
      - [Optional parameter for column mapping](#optional-parameter-for-column-mapping)
      - [Pipeline specific parameters](#pipeline-specific-parameters)
        - [Parameters on the datastore level](#parameters-on-the-datastore-level)
        - [Parameters on the dataset level](#parameters-on-the-dataset-level)
    - [Creating a `trigger_info.yaml`](#creating-a-trigger_infoyaml)
    - [\[Optionally\] Provide a mapping](#optionally-provide-a-mapping)
  - [Using storage triggers](#using-storage-triggers)
- [How does it work?](#how-does-it-work)
  - [Configuration of environments](#configuration-of-environments)
- [Requirements](#requirements)
- [Providers](#providers)
- [Modules](#modules)
- [Resources](#resources)
- [Inputs](#inputs)
- [Outputs](#outputs)



## How to schedule a new pipeline?

To get a working triggered pipeline, you need to:

1. Create a parameterized pipeline in [this repository](https://dev.azure.com/NIBC/Data%20Platform/_git/adf-cdp-pipelines), for requirements see [this section](#creating-a-parameterized-pipeline)
2. [Create a `trigger_info.yaml`](#creating-a-trigger_infoyaml)
3. [\[Optionally\] Provide a mapping](#optionally-provide-a-mapping)


There is an exception to this. If you want to have your triggers activated on a storage event, there is a separate approach. For example if somehow, we get files to land on our internal ingestion storage acccount (perhaps we put them there ourselves through another pipeline), we can trigger an ADF pipeline run.

### Using scheduled triggers

#### Creating a parameterized pipeline

Using ADF, create a pipeline and define parameters to be filled by this repository. You can find an
explanation on pipeline parameters [here](https://learn.microsoft.com/en-us/azure/data-factory/concepts-parameters-variables).

These are some parameters you want to define:

##### Required parameters for each pipeline
These are 3 parameters that identify your dataset which must be defined for all pipelines:

1. `datastore_name` - the name of the datastore you get data from, fill out with `camel_case`, like `spaargids_be`
2. `environment` - the name of the environment you're pulling data from, like `dev`, `tst`, `acp`, `prd`
3. `dataset_name` - the name of the dataset


They are used to determine:

- schema and tables names in Unity Catalog - `<datastore_name>_<environment>.<dataset_name>`
- folder structure in the storage account - `datastore_<datastore_name>/environment_<environment>/dataset_<dataset_name>`
- folder structure in this repository

```
.
└── env
    └── <subscription>
        └── datastores
            └── <datastore_name>
                └── <environment>
                    ├── trigger_info.yaml
                    └── mappings
                        ├── <dataset_name>.json
                        └── <dataset_name>.json
```

##### Optional parameter for column mapping

Your pipeline may include setting a column mapping. You can provide this column
dynamically, using the `column_mapping` parameter. If you:

- set that as a pipeline parameters and
- use it in the mapping section of your pipeline's copy activity and
- add a valid mapping json file adhering to the folder structure described above

Your JSON will be used by data factory to dynamically set your mapping. This
way you can more easily re-use your pipeline for multiple datasets that each may
have different mappings.

It is a bit tricky to create a valid mapping yourself, so to create a new mapping,
we recommend to:

- temporarily make a copy of your pipeline
- let ADF help you create a mapping
- run the pipeline (with debug) to make sure your mapping work as expected
- open the JSON view of the pipeline
- Copy the mapping part from the JSON, into this repository

You can check the existing mappings to see what the end-result should look like.


##### Pipeline specific parameters

For each pipeline, you may need additional bits of information, this can be provided separately
and could be on two different levels:

###### Parameters on the datastore level

Examples could be:

- API base URL
- database connection string secret name (referring to a keyvault secret)
- ..

Provide these as follows in the `trigger_info.yaml`:

```yaml
datastore_parameters:
  base_url: https://www.spaargids.be/download/
```


###### Parameters on the dataset level

Examples could be:

- API relative URL
- database schema
- database table
- ..

Provide these as follows in the `trigger_info.yaml`:

```yaml
datasets:
  - name: rentes_nibc_all
    dataset_parameters:
      relative_url: rentes_nibc_all.xml
```


#### Creating a `trigger_info.yaml`

You will want to have one `trigger_info.yaml` file for each datastore, for each environment of
this datastore for which you want to pull data from.

You'll want to place it in the folder structure described
[above](#required-parameters-for-each-pipeline).

Inside this yaml, is all information needed to create your pipeline trigger, as shown in this example below:


```yaml
pipeline_name: pl_anonymous_http_xml_spaargids_be

schedule:
  frequency: Day
  interval: 1
  # You can also provide list for hour of the day and minute within that hour
  # to steer the timing of the trigger, the following would schedule your trigger
  # at noon:
  # hours: [12]
  # minutes: [0]

datastore_parameters:
  base_url: https://www.spaargids.be/download/

datasets:
  - name: rentes_nibc_all
    dataset_parameters:
      relative_url: rentes_nibc_all.xml
  - name: termijnrekening_nibc
    dataset_parameters:
      relative_url: termijnrekening_NIBC.xml

```

#### \[Optionally\] Provide a mapping

You can provide a column mapping, as is described [above](#optional-parameter-for-column-mapping).

Example mapping generated with the help of ADF:

```json
{
    "type": "TabularTranslator",
    "mappings": [
        {
            "source": {
                "path": "$['termijnrekening']['datum']"
            },
            "sink": {
                "name": "datum",
                "type": "String"
            }
        },
        {
            "source": {
                "path": "$['termijnrekening']['generated_on']"
            },
            "sink": {
                "name": "generated_on",
                "type": "String"
            }
        },
        {
            "source": {
                "path": "['@waarde']"
            },
            "sink": {
                "name": "waarde",
                "type": "String"
            }
        },
        {
            "source": {
                "path": "['product']"
            },
            "sink": {
                "name": "product",
                "type": "String"
            }
        }
    ],
    "collectionReference": "$['termijnrekening']['termijn']",
    "mapComplexValuesToString": true
}
```

### Using storage triggers

If you want to create a storage trigger instead of a scheduled trigger, you'll need to create a `storage_trigger_info.yaml` file in the same location you would otherwise place the `trigger_info.yaml` file.

An example storage_trigger_info file looks like this:

```yaml
description: |
  FRL run for pipeline X will create csv files
  When those are created, this trigger will fire

storage_trigger:
  blob_path_begins_with: "/internal/blobs/datastore_frl/"
  blob_path_ends_with: ".csv"

pipeline_name: pl_blob_escaped_new_line_csv_cloud
pipeline_parameters:
  datastore_name: frl
  dataset_name: "@replace(triggerBody().fileName, '.csv', '')"

```

More information on storage event triggers in ADF can be found [here](https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory)

## How does it work?

This codebase uses Terraform to deploy and manage the triggers in an Azure Data Factory. The [main.tf](./main.tf) file is used to:

1. Read trigger configurations from the `trigger_info.yaml` files as described [here](#creating-a-trigger_infoyaml)
2. Optionally add mappings from the JSON files
3. Create Azure Data Factory triggers objects based on the configuration

Deployment is done using the Azure DevOps pipeline [available here](./azure-pipelines.yaml)


### Configuration of environments

Most likely the triggers need to be deployed to multiple environments. Every environment has its own configuration for Terraform.
Inputs for terraform, and for the DevOps pipelines can be found in subscription specific folder like the one [here](./env/cdp-preprd/vars/)

There are two files in each environment:
- **backend.tfvars** This file contains all the configuration needed to connect to the Terraform backend (StateFile). This file can be passed to `terraform init -backend-config`
- **vars.tfvars** This files contains configuration to locate the Azure Data Factory


<!-- BEGIN_TF_DOCS -->
# Automatically generated documentation
Below documentation is generated using the [Terraform docs](https://terraform-docs.io/user-guide/introduction/) tooling.

## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | >= 1.4.2 |
| <a name="requirement_azurerm"></a> [azurerm](#requirement\_azurerm) | 4.34.0 |

## Providers

| Name | Version |
|------|---------|
| <a name="provider_azurerm"></a> [azurerm](#provider\_azurerm) | 4.34.0 |

## Modules

No modules.

## Resources

| Name | Type |
|------|------|
| [azurerm_data_factory_trigger_blob_event.this](https://registry.terraform.io/providers/hashicorp/azurerm/4.34.0/docs/resources/data_factory_trigger_blob_event) | resource |
| [azurerm_data_factory_trigger_schedule.this](https://registry.terraform.io/providers/hashicorp/azurerm/4.34.0/docs/resources/data_factory_trigger_schedule) | resource |
| [azurerm_data_factory.this](https://registry.terraform.io/providers/hashicorp/azurerm/4.34.0/docs/data-sources/data_factory) | data source |
| [azurerm_storage_account.ingestion](https://registry.terraform.io/providers/hashicorp/azurerm/4.34.0/docs/data-sources/storage_account) | data source |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_adf_name"></a> [adf\_name](#input\_adf\_name) | Name of data factory instance to deploy schedules | `string` | n/a | yes |
| <a name="input_adf_resource_group_name"></a> [adf\_resource\_group\_name](#input\_adf\_resource\_group\_name) | Resource group name of data factory instance to deploy schedules | `string` | n/a | yes |
| <a name="input_env_path"></a> [env\_path](#input\_env\_path) | The path where vars and configs are. Eg, env/prd etc. | `string` | n/a | yes |
| <a name="input_storage_account_name"></a> [storage\_account\_name](#input\_storage\_account\_name) | Name of storage account used for storage triggers | `string` | n/a | yes |
| <a name="input_storage_account_resource_group_name"></a> [storage\_account\_resource\_group\_name](#input\_storage\_account\_resource\_group\_name) | Name of resource group containing storage account used for storage triggers | `string` | n/a | yes |

## Outputs

No outputs.
<!-- END_TF_DOCS -->
